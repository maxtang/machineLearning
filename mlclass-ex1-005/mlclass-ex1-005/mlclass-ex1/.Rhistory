library(MASS)
library(ISLR)
install.packages('ISLR')
library(ISLR)
names(Boston)
?Boston
plot(medv~lstat,Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
### Nonlinear terms and Interactions
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
###Qualitative predictors
fix(Carseats)
names(Carseats)
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
###Writing R functions
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
require(ISLR)
names(Smarket)
summary(Smarket)
?Smarket
pairs(Smarket,col=Smarket$Direction)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial)
summary(glm.fit)
glm.probs=predict(glm.fit,type="response")
glm.probs[1:5]
glm.pred=ifelse(glm.probs>0.5,"Up","Down")
attach(Smarket)
table(glm.pred,Direction)
mean(glm.pred==Direction)
train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
glm.fit=glm(Direction~Lag1+Lag2,
data=Smarket,family=binomial, subset=train)
glm.probs=predict(glm.fit,newdata=Smarket[!train,],type="response")
glm.pred=ifelse(glm.probs >0.5,"Up","Down")
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
106/(76+106)
summary(glm.fit)
require(MASS)
?lda
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
lda.fit
plot(lda.fit)
Smarket.2005=subset(Smarket,Year==2005)
lda.pred=predict(lda.fit,Smarket.2005)
lda.pred[1:5,]
class(lda.pred)
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)
library(class)
?knn
attach(Smarket)
ls
ls()
objects(7)
Lag1
Xlag=cbind(Lag1,Lag2)
xlag[1:5,]
Xlag[1:5,]
train=Year<2005
knn.pred=knn(Xlag[train,],Xlag[!train,],Direction[train],k=1)
table(knn.pred,Direction[!train])
mean(knn.pred==Direction[!train])
Smarket
View(Smarket.2005)
View(Smarket.2005)
require(ISLR)
require(boot)
?cv.glm
plot(mpg~horsepower,data=Auto)
glm.fit=glm(mpg~horsepower, data=Auto)
cv.glm(Auto,glm.fit)$delta #pretty slow (doesnt use formula (5.2) on page 180)
loocv=function(fit){
h=lm.influence(fit)$h
mean((residuals(fit)/(1-h))^2)
}
loocv(glm.fit)
cv.error=rep(0,5)
degree=1:5
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error[d]=loocv(glm.fit)
}
plot(degree,cv.error,type="b")
plot(degree,cv.error,type="b")
## 10-fold CV
cv.error10=rep(0,5)
for(d in degree){
glm.fit=glm(mpg~poly(horsepower,d), data=Auto)
cv.error10[d]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
lines(degree,cv.error10,type="b",col="red")
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Portfolio$X,Portfolio$Y)
## What is the standard error of alpha?
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
alpha.fn(Portfolio,1:100)
Portfolio
?with
Portfolio[1,]
alpha(Portfolio[1,])
Portfolio[1,1]
Portfolio[1,2]
alpha(Portfolio[1,1],Portfolio[1,2])
set.seed(1)
alpha.fn (Portfolio,sample(1:100,100,replace=TRUE))
Portfolio[1:10,]
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out
plot(boot.out)
plot(boot.out)
x=rnorm (100)
y=rnorm (100)
plot(x,y,col =" green ")
install.packages(knitr)
install.packages("knitr")
require(knitr)
library(ISLR)
summary(Hitters)
Hitters=na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
?sum
library(leaps)
install.packages("leaps")
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
plot(regfit.full,scale="Cp")
coef(regfit.full,10)
library(ISLR)
summary(Hitters)
library(leaps)
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
val.errors=rep(NA,19)
?rep
val.errors
?model.matrix
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
x.test
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
regfit.fwd
?regsubsets
x.test
regfit.fwd
coef(regfit.fwd)
?coef
coef(regfit.fwd, id=1)
coef(regfit.fwd, id=2)
coef(regfit.fwd, id=3)
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
coef
}
coefi=coef(regfit.fwd,id=1)
coefi
pred=x.test[,names(coefi)]%*%coefi
pred
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
?model.matrix
x.test
pred
names(coefi)
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
mat[,names(coefi)]%*%coefi
}
?table
nrow(Hitters)
rep(1:10,length=nrow(Hitters))
table(folds)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
install.packages("glmnet")
?cv.glmnet
?cv
library(glmnet)
?cv.glmnet
require(ISLR)
require(tree)
attach(Carseats)
hist(Sales)
Sales
hist(Sales)
High
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats, High)
View(Carseats)
tree.carseats=tree(High~.-Sales,data=Carseats)
require(ISLR)
require(tree)
install.packages("tree")
require(tree)
tree.carseats=tree(High~.-Sales,data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(1011)
train=sample(1:nrow(Carseats),250)
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
plot(tree.carseats);text(tree.carseats,pretty=0)
?predict
tree.pred=predict(tree.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))
?table
fix(tree.pred)
Carseats[-train,]
view(tree.pred)
View(Carseats)
View(tree.pred)
with(Carseats[-train,],table(tree.pred,High))
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats);text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats[-train,],type="class")
with(Carseats[-train,],table(tree.pred,High))
(72+32)/150
require(randomForest)
require(MASS)
install.packages("MASS")
install.packages("MASS")
install.packages("randomForest")
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
?Boston
train=sample(1:nrow(Boston),300)
rf.boston=randomForest(medv~.,data=Boston,subset=train)
rf.boston
oob.err=double(13)
test.err=double(13)
?double
for(mtry in 1:13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
cat(mtry," ")
}
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
install.packages(gbm)
summary(boost.boston)
install.packages("gbm")
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
summary(boost.boston)
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
n.trees=seq(from=100,to=10000,by=100)
predmat=predict(boost.boston,newdata=Boston[-train,],n.trees=n.trees)
dim(predmat)
predmat
View(predmat)
?apply
plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
abline(h=min(test.err),col="red")
k=read.csv("C:\\Documents and Settings\\Max\\Desktop\\temp\\kaggle\\train_v2.csv")
require(randomForest)
dim(k)
?sample
train = sample(1:nrow(k),65471)
k[1,]
train
rf.k = randomForest(loss ~ ., data=k, subset=train)
rf.k
?randomForest
rf.k = randomForest(loss ~ ., data=k, subset=train, na.action=na.omit)
rf.k
rf.k = randomForest(loss ~ ., data=k, subset=train)
sessionInfo()
rf.k = randomForest(loss ~ ., data=k, subset=train)
cd('C:\Documents and Settings\Max\Desktop\git\ml-005\mlclass-ex1-005\mlclass-ex1-005\mlclass-ex1')
setwd("C:/Documents and Settings/Max/Desktop/git/ml-005/mlclass-ex1-005/mlclass-ex1-005/mlclass-ex1")
?readcsv
ml=read.csv('ex1data2.txt')
ml
View(ml)
?read.csv
ml=read.csv('ex1data2.txt', head=F)
View(ml)
?lm
lm(V3~., ml)
?predict
predict(lm(V3~., ml), c(1650, 3))
c(1650,3)
data.frame(c(1650,3))
predict(lm(V3~., ml), data.frame(c(1650, 3)))
?object
?data.frame
predict(lm(V3~., ml), data.frame(c('V1','V2'),c(1650, 3)))
predict(lm(V3~., ml), data.frame(V1=1650, V2=3))
lm(V3~., ml)
89597.0+139.2*1650-8738*3
plot(lm(V3~., ml))
plot(lm(V3~., ml))
summary(lm(V3~., ml))
summary(lm(V3~., ml))
library(scatterplot3d)
install.packages("scatterplot3d")
library(scatterplot3d)
attach(ml)
s3d <- (V1,V2,V3)
s3d <- scatterplot3d(V1,V2,V3)
install.packages("Rcmdr")
library("Rcmdr")
scatter3d(V1,V2,V3)
?scatter3d
scatter3d(V3 ~V1+V2)
?option
option(scipen=10)
options(scipen=10)
scatter3d(V3 ~V1+V2)
summary(lm(V3~V1+V2))
V1*139.2
V1*139.2-8738*V2-V3
89597.9 + V1*139.2-8738*V2-V3
(89597.9 + V1*139.2-8738*V2-V3)^2
sum((89597.9 + V1*139.2-8738*V2-V3)^2)
sum((0.109740 + V1*165.382735-0.305253*V2-V3)^2)
sum((0.452942 + V1*165.381724-0.902704*V2-V3)^2)
summary(ml)
?stddev
?std
?summary
sd(ml)
V1-mean(V1)
(V1-mean(V1))/sd(V1)
(V2-mean(V2))/sd(V2)
